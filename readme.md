# EU Regulation Processing & Retrieval Pipeline

A modular pipeline for scraping, processing, summarizing, vectorizing, and querying European Union regulations.

## ğŸ§© Project Structure

```
Project/
â”œâ”€â”€ data/                    # All input/output files
â”‚   â”œâ”€â”€ pdfs/                # Downloaded PDF files
â”‚   â”œâ”€â”€ processed_documents/ # Final summarized and sanitized .txt files 
â”‚   â”œâ”€â”€ raw/                 # Raw text files from EUR-Lex
â”‚   â”œâ”€â”€ vectorstore/         # Chroma DB files
â”‚   â”œâ”€â”€ eurlex_results.json  # Metadata with CELEX ID, title, date, and link
â”‚   â””â”€â”€ eurlex_ro.json       # Metadata with CELEX ID, title, date, and link for PDF files
â”œâ”€â”€ scripts/                 # Modular processing scripts
â”‚   â”œâ”€â”€ build_vector_db.py
â”‚   â”œâ”€â”€ clean_pdfs.py
â”‚   â”œâ”€â”€ extract_pdf.py
â”‚   â”œâ”€â”€ extract_result_text.py
â”‚   â”œâ”€â”€ parse_all_results.py
â”‚   â”œâ”€â”€ parse_romanian_results.py
â”‚   â”œâ”€â”€ sanitize.py
â”‚   â””â”€â”€ summarize.py
â”œâ”€â”€ config.py                # Central config for paths, API keys, constants
â”œâ”€â”€ main_pipeline.py         # Runs the full pipeline from scrape to vector DB
â”œâ”€â”€ gui.py                   # Streamlit interface
â”œâ”€â”€ rag_interface.py         # Loads vector DB, allows user to ask questions
â”œâ”€â”€ readme.md                # You are reading me right now!
â””â”€â”€ requirements.txt         # Python dependencies
```

## âš™ï¸ Setup

1. **Create environment**

```bash
python -m venv venv
venv\Scripts\activate   # On Windows
pip install -r requirements.txt
```

2. **Set your OpenAI API key**
- Set it inside `config.py` as `OPENAI_API_KEY = "sk-..."`


## ğŸš€ How to Use

### Option 1: Use the interface
### Step 1: Run the interface
```bash
streamlit run gui.py
```
This will:
- Launch the interace in the browser
- Either build/update the database by pressing "Run Processing Pipeline"
- Or start prompting the chatbot directly

### Step 2:
- Enjoy!

### Option 2: Use the terminal
### Step 1: Build or update the dataset
```bash
python main_pipeline.py
```
This will:
- Scrape new metadata from EUR-Lex
- Extract HTML content
- Scrape PDF Documents
- Extract the text content of the PDFs
- Sanitize and summarize documents (skips already processed ones)
- Save outputs in the folder processed_documents
- Build or update the vector database

### Step 2: Query the knowledge base
```bash
python rag_interface.py
```
- Prompts the user for a question
- Automatically detects the correct category
- Retrieves context and generates an answer using OpenAI


## ğŸ§  Features
- **Skips already processed files** to minimize API usage
- **Modular scripts** for each step
- **Token-aware summarization** (600 tokens)
- **Filtered context retrieval** using Chroma and LangChain


## ğŸ“¦ Dependencies
- `openai`
- `langchain`
- `langchain-chroma`
- `beautifulsoup4`
- `selenium`
- `webdriver-manager`


## ğŸ“Œ Notes
- The scraping target is the EUR-Lex website filtered to show only regulations.
- The system assumes filenames are based on CELEX IDs (e.g. `32021R0123.txt`).


