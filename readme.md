# EU Regulation Processing & Retrieval Pipeline

A modular pipeline for scraping, processing, summarizing, vectorizing, and querying European Union regulations.

## ğŸ§© Project Structure

```
Project/
â”œâ”€â”€ data/                   # All input/output files
â”‚   â”œâ”€â”€ raw/                # Raw text files from EUR-Lex
â”‚   â”œâ”€â”€ summarized/         # Final summarized .txt files grouped by category
â”‚   â”œâ”€â”€ vectorstore/        # Chroma DB files
â”‚   â””â”€â”€ eurlex_results.json # Metadata with CELEX ID, title, date, and link
â”œâ”€â”€ scripts/                # Modular processing scripts
â”‚   â””â”€â”€ build_vector_db.py
â”‚   â”œâ”€â”€ categorize.py
â”‚   â”œâ”€â”€ extract_result_text.py
â”‚   â”œâ”€â”€ parse_all_results.py
â”‚   â”œâ”€â”€ sanitize.py
â”‚   â”œâ”€â”€ summarize.py
â”œâ”€â”€ config.py               # Central config for paths, API keys, constants
â”œâ”€â”€ main_pipeline.py        # Runs the full pipeline from scrape to vector DB
â”œâ”€â”€ gui.py                  # Streamlit interface
â”œâ”€â”€ rag_interface.py        # Loads vector DB, allows user to ask questions
â”œâ”€â”€ readme.md               # You are reading me right now!
â””â”€â”€ requirements.txt        # Python dependencies
```

## âš™ï¸ Setup

1. **Create environment**

```bash
python -m venv venv
venv\Scripts\activate   # On Windows
pip install -r requirements.txt
```

2. **Set your OpenAI API key**
- Set it inside `config.py` as `OPENAI_API_KEY = "sk-..."`


## ğŸš€ How to Use

### Step 1: Run the interface
```bash
streamlit run gui.py
```
This will:
- Launch the interace in the browser
- There will be 2 actions displayed at first
- First: build the database (parse information from EUR-Lex)
- Second: run the chatbot for prompting

### Step 2:
- Start prompting the model
- Enjoy!

### Alternatively, you can
### Run the code without interface

### Step 1: Build or update the dataset
```bash
python main_pipeline.py
```
This will:
- Scrape new metadata from EUR-Lex
- Extract HTML content
- Categorize, sanitize and summarize documents (skips already processed ones)
- Save outputs in structured folders
- Build or update the vector database

### Step 2: Query the knowledge base
```bash
python rag_interface.py
```
- Prompts the user for a question
- Automatically detects the correct category
- Retrieves context and generates an answer using OpenAI


## ğŸ§  Features
- **Skips already processed files** to minimize API usage
- **Modular scripts** for each step
- **Token-aware summarization** (600 tokens)
- **Filtered context retrieval** using Chroma and LangChain


## ğŸ“¦ Dependencies
- `openai`
- `langchain`
- `langchain-chroma`
- `beautifulsoup4`
- `selenium`
- `webdriver-manager`


## ğŸ“Œ Notes
- The scraping target is the EUR-Lex website filtered to show only regulations.
- The system assumes filenames are based on CELEX IDs (e.g. `32021R0123.txt`).


